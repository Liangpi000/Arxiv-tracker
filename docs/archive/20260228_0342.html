<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-28 03:42</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260228_0342</div>
    <div class="row"><div class="card">
<div class="title">MediX-R1: Open Ended Medical Reinforcement Learning</div>
<div class="meta-line">Authors: Sahal Shaji Mullappilly, Mohammed Irfan Kurpath, Omair Mohamed, Mohamed Zidan, Fahad Khan, Salman Khan, Rao Anwer, Hisham Cholakkal</div>
<div class="meta-line">First: 2026-02-26T18:59:46+00:00 · Latest: 2026-02-26T18:59:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MediX-R1：开放式的医疗强化学习</div>
<div class="mono" style="margin-top:8px">我们介绍了MediX-R1，这是一种针对医疗多模态大型语言模型（MLLMs）的开放式强化学习（RL）框架，能够提供基于临床的、自由形式的答案，超越了多项选择格式。MediX-R1 使用基于组的RL对基础视觉-语言骨干进行微调，并结合了针对医疗推理的复合奖励：基于LLM的准确度奖励，用于判断语义正确性并做出严格的YES/NO决策；基于医学嵌入的语义奖励，以捕捉同义词和术语变体；以及轻量级的格式和模态奖励，以确保可解释的推理和模态识别。这种多信号设计为传统可验证或仅基于MCQ的奖励无法提供稳定、信息丰富的反馈的开放式输出提供了支持。为了衡量进展，我们提出了一种统一的评估框架，用于文本和图像+文本任务，该框架使用LLM作为评判者替代脆弱的字符串重叠度量，以捕捉语义正确性、推理和上下文对齐。尽管仅使用约51,000个指令示例，MediX-R1 在标准的医疗LLM（仅文本）和VLM（图像+文本）基准测试中取得了优异的成绩，超越了强大的开源基线，并在开放式临床任务上取得了特别大的进步。我们的结果表明，使用全面的奖励信号和基于LLM的评估的开放式RL是一种通往可靠的多模态模型中医疗推理的实际路径。我们的训练模型、精选数据集和源代码可在https://medix.cvmbzuai.com 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MediX-R1 is an open-ended RL framework for medical MLLMs that fine-tunes a vision-language backbone with a composite reward including LLM-based accuracy, medical embedding semantic, and lightweight format rewards. It uses a reference-based LLM-as-judge for evaluation, achieving excellent results on medical LLM and VLM benchmarks, especially in open-ended clinical tasks, surpassing strong open-source baselines.</div>
<div class="mono" style="margin-top:8px">MediX-R1 是一个面向 MLLMs 的开放性 RL 框架，能够生成自由形式的医疗答案。它通过 Group Based RL 和一个综合奖励系统（包括 LLM 基准准确性、医学嵌入语义以及轻量级格式/模态奖励）来微调视觉语言骨干网络。这种方法为开放性输出提供了稳定的反馈。MediX-R1 在标准的医疗 LLM 和 VLM 基准测试中表现出色，特别是在开放性临床任务上。提出了一种基于参考的 LLM 作为评判者的统一评估框架来衡量进展。模型和数据集已公开可用。</div>
</details>
</div>
<div class="card">
<div class="title">SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation</div>
<div class="meta-line">Authors: Vaibhav Agrawal, Rishubh Parihar, Pradhaan Bhat, Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-26T18:59:05+00:00 · Latest: 2026-02-26T18:59:05+00:00</div>
<div class="meta-line">Comments: Project page: https://seethrough3d.github.io. Accepted at CVPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23359v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23359v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://seethrough3d.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SeeThrough3D：基于遮挡感知的3D控制在文本到图像生成中的应用</div>
<div class="mono" style="margin-top:8px">我们识别出遮挡推理是3D布局条件生成中一个基本但被忽视的方面。它对于合成具有深度一致几何结构和比例的部分遮挡物体至关重要。虽然现有的方法可以生成遵循输入布局的逼真场景，但它们往往无法准确建模物体间的遮挡。我们提出了SeeThrough3D，一种用于3D布局条件生成的模型，该模型明确建模了遮挡。我们引入了一种遮挡感知的3D场景表示（OSCR），其中物体以透明的3D盒子形式置于虚拟环境中，并从期望的摄像机视角进行渲染。透明度编码了隐藏的物体区域，使模型能够推理遮挡，而渲染的视角则在生成过程中提供了明确的摄像机控制。我们通过引入从我们渲染的3D表示中提取的一组视觉标记，对一个预训练的基于流的文本到图像图像生成模型进行条件化。此外，我们应用掩码自注意力来准确地将每个物体边界框与其相应的文本描述绑定，从而实现多个物体的准确生成，而不会出现物体属性混杂。为了训练该模型，我们构建了一个包含多种具有强烈物体间遮挡的合成数据集。SeeThrough3D能够有效泛化到未见过的物体类别，并实现具有真实遮挡和一致摄像机控制的精确3D布局控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the issue of occlusion reasoning in text-to-image generation, which is crucial for creating scenes with depth-consistent geometry and scale. The proposed SeeThrough3D model introduces an occlusion-aware 3D scene representation (OSCR) that uses translucent 3D boxes and rendered viewpoints to model occlusions. Key findings include the model&#x27;s ability to generate scenes with precise inter-object occlusions and realistic camera control, even for unseen object categories. The model is trained on a synthetic dataset with diverse multi-object scenes and demonstrates effective generalization and control over 3D layouts.</div>
<div class="mono" style="margin-top:8px">研究旨在改善文本到图像生成中对3D布局的遮挡处理。SeeThrough3D提出了一种遮挡感知的3D场景表示（OSCR），使用半透明的3D盒子和渲染视角来建模遮挡。模型通过从3D表示中提取视觉标记来条件化预训练的文本到图像生成器，并使用掩蔽自注意力将每个物体边界框与其对应的文本描述准确绑定，从而实现精确的遮挡和相机控制。实验表明，SeeThrough3D能够生成具有现实遮挡和一致相机控制的场景，即使是对未见过的物体类别也是如此。</div>
</details>
</div>
<div class="card">
<div class="title">A Dataset is Worth 1 MB</div>
<div class="meta-line">Authors: Elad Kimchi Shoshani, Leeyam Gabay, Yedid Hoshen</div>
<div class="meta-line">First: 2026-02-26T18:59:03+00:00 · Latest: 2026-02-26T18:59:03+00:00</div>
<div class="meta-line">Comments: 23 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23358v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一个数据集值1 MB</div>
<div class="mono" style="margin-top:8px">数据集服务器经常需要向许多客户端分发相同的大型负载，导致巨大的通信成本。由于客户端通常运行在不同的硬件和软件框架上，传输预训练模型往往是不可行的；相反，代理需要原始数据来训练自己的任务特定模型。虽然数据集蒸馏试图压缩训练信号，但当前的方法难以扩展到高分辨率数据，很少能实现足够小的文件。在本文中，我们提出了一种名为Pseudo-Labels as Data (PLADA)的方法，该方法完全消除了像素传输。我们假设代理预先加载了一个大型、通用、未标记的参考数据集（例如，ImageNet-1K，ImageNet-21K），并通过仅传输特定图像的类别标签来传达新任务。为了解决参考数据集和目标数据集之间的分布不匹配，我们引入了一种剪枝机制，该机制过滤参考数据集，仅保留与目标任务最相关的图像标签。这个选择过程同时最大化了训练效率并最小化了传输负载。在10个不同的数据集上的实验表明，我们的方法可以以小于1 MB的负载转移任务知识，同时保持高分类准确性，为高效的数据集服务提供了一个有前景的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of efficiently distributing large datasets to multiple clients by proposing PLADA, which eliminates the need to transmit pixel data. Instead, it sends only class labels for specific images, reducing the payload to less than 1 MB. The method uses a preloaded reference dataset and a pruning mechanism to select the most semantically relevant images, ensuring high classification accuracy while minimizing communication costs.</div>
<div class="mono" style="margin-top:8px">本文解决了向多个客户端分发大数据集时高昂的通信成本问题。它提出了PLADA方法，通过只发送特定图像的类别标签来消除传输像素数据的需要。通过筛选参考数据集，只保留与目标任务最相关的图像标签，PLADA减少了传输负载同时保持了高分类准确性。在10个不同数据集上的实验表明，PLADA可以使用小于1 MB的负载传输任务知识。</div>
</details>
</div>
<div class="card">
<div class="title">SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport</div>
<div class="meta-line">Authors: Simon Roschmann, Paul Krzakala, Sonia Mazelet, Quentin Bouniot, Zeynep Akata</div>
<div class="meta-line">First: 2026-02-26T18:55:06+00:00 · Latest: 2026-02-26T18:55:06+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23353v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23353v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SOTAlign：通过最优传输实现单模态视觉和语言模型的半监督对齐</div>
<div class="mono" style="margin-top:8px">柏拉图表征假设认为，训练在不同模态上的神经网络会趋向于共享一个世界统计模型。近期的工作通过使用对比损失和大量配对样本将冻结的预训练视觉和语言模型对齐，但通常依赖于对比损失和数百万配对样本。在本文中，我们探讨是否可以在较少监督的情况下实现有意义的对齐。我们引入了一种半监督设置，在该设置中，使用少量的图像-文本配对数据和大量未配对数据对预训练的单模态编码器进行对齐。为了解决这一挑战，我们提出了SOTAlign，这是一种两阶段框架，首先使用线性教师从有限的配对数据中恢复粗略的共享几何结构，然后通过基于最优传输的发散性在未配对样本上细化对齐，该发散性可以转移关系结构而不过度约束目标空间。与现有的半监督方法不同，SOTAlign有效地利用了未配对的图像和文本，学习跨数据集和编码器对的稳健联合嵌入，并显著优于监督和半监督基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to achieve alignment between vision and language models with less supervision. SOTAlign, a two-stage framework, first uses a small number of image-text pairs to establish a coarse shared geometry, then refines the alignment on unpaired data using an optimal-transport-based divergence. This method outperforms both supervised and semi-supervised baselines, demonstrating the effectiveness of leveraging unpaired data for alignment.</div>
<div class="mono" style="margin-top:8px">研究旨在通过较少的监督实现视觉和语言模型之间的有意义对齐。SOTAlign 是一个两阶段框架，首先使用少量的图像-文本对来恢复粗略的共享几何结构，然后通过最优传输基的发散性在未配对样本上进行对齐细化。该方法在监督和半监督基线中表现出色，证明了利用未配对数据进行对齐的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Scale Can&#x27;t Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning</div>
<div class="meta-line">Authors: Amita Kamath, Jack Hessel, Khyathi Chandu, Jena D. Hwang, Kai-Wei Chang, Ranjay Krishna</div>
<div class="meta-line">First: 2026-02-26T18:54:06+00:00 · Latest: 2026-02-26T18:54:06+00:00</div>
<div class="meta-line">Comments: TACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23351v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23351v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., &quot;at the game today!&quot; is a more likely caption than &quot;a photo of 37 people standing behind a field&quot;. We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>规模无法克服语用学：报告偏差对视觉语言推理的影响</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）缺乏推理能力的问题一直是研究讨论的焦点。我们认为这种行为源于其训练数据中的报告偏差。也就是说，人们默认描述视觉内容时会省略一些监督某些类型推理所需的隐含信息；例如，“今天在比赛！”比“一张37个人站在田野后面的图片”更可能作为描述。我们通过语用学理论的视角，研究了流行的VLMs OpenCLIP、LLaVA-1.5和Molmo的数据基础，发现报告偏差导致在空间、时间、否定和计数这四种推理技能上缺乏足够的表示，尽管这些语料库是大规模的，或者合成生成的。通过一组精心策划的基准测试，我们证明：(i) VLMs在由报告偏差抑制的上述类型推理上表现不佳；(ii) 与普遍认为的相反，增加数据量、模型规模和多语言训练并不会默认产生这些技能；但令人鼓舞的是，(iii) 特别收集用于获取隐含信息的注解是有效的。我们的研究结果强调了需要更故意的数据集策划方法，而不是依赖规模来产生推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates the reasoning capabilities of Vision-Language Models (VLMs) and finds that their performance is hindered by a reporting bias in their training data, which omits necessary tacit information for certain types of reasoning. Despite the large scale of the datasets, scaling the data or model size does not improve these reasoning skills. However, incorporating specific annotations that capture tacit information improves performance. This suggests that intentional data curation is crucial for enhancing reasoning capabilities in VLMs.</div>
<div class="mono" style="margin-top:8px">研究探讨了报告偏见对Vision-Language模型（如OpenCLIP、LLaVA-1.5和Molmo）推理能力的影响。通过使用语用学理论分析训练数据，研究发现报告偏见导致空间、时间、否定和计数推理技能的不足表示，尽管数据集规模庞大。实验表明，这些模型在这些类型的推理方面表现不佳，单纯增加数据或模型规模并不能改善这些技能。然而，通过特定注释来捕捉隐含信息可以提高模型在这些方面的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?</div>
<div class="meta-line">Authors: Tilemachos Aravanis, Vladan Stojnić, Bill Psomas, Nikos Komodakis, Giorgos Tolias</div>
<div class="meta-line">First: 2026-02-26T18:45:33+00:00 · Latest: 2026-02-26T18:45:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23339v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>检索与分割：少量示例足以弥合开放词汇分割中的监督缺口吗？</div>
<div class="mono" style="margin-top:8px">开放词汇分割（OVS）将视觉语言模型（VLMs）的零样本识别能力扩展到像素级预测，使模型能够根据文本提示分割任意类别。尽管取得了进展，但由于训练VLMs所使用的粗略图像级监督和自然语言的语义模糊性，OVS仍落后于完全监督的方法。我们通过引入一种少量示例设置，将文本提示与像素标注图像的支持集相结合，来解决这些限制。在此基础上，我们提出了一种检索增强的测试时适配器，通过融合文本和视觉支持特征学习一种轻量级的、针对每张图像的分类器。与依赖于后期手工融合的先前方法不同，我们的方法进行学习的、针对每个查询的融合，实现了模态之间的更强协同作用。该方法支持不断扩展的支持集，并适用于细粒度任务，如个性化分割。实验表明，我们显著缩小了零样本和监督分割之间的差距，同时保留了开放词汇的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of open-vocabulary segmentation (OVS) by proposing a few-shot setting that combines textual prompts with pixel-annotated images. The authors introduce a retrieval-augmented test-time adapter to learn a lightweight, per-image classifier by fusing textual and visual support features, achieving better synergy between modalities than prior methods. Experiments demonstrate that this approach significantly reduces the gap between zero-shot and supervised segmentation while maintaining open-vocabulary capabilities.</div>
<div class="mono" style="margin-top:8px">本文通过将文本提示与像素标注图像相结合的少量样本设置，解决了开放词汇分割（OVS）的局限性。作者引入了一种检索增强的测试时适配器，通过融合文本和视觉支持特征来学习轻量级分类器，这种方法在模态间协同作用方面优于先前的方法。实验表明，这种方法显著缩小了零样本和监督分割之间的差距，同时保持了开放词汇的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Differentiable Zero-One Loss via Hypersimplex Projections</div>
<div class="meta-line">Authors: Camilo Gomez, Pengyang Wang, Liansheng Tang</div>
<div class="meta-line">First: 2026-02-26T18:41:31+00:00 · Latest: 2026-02-26T18:41:31+00:00</div>
<div class="meta-line">Comments: To appear in PAKDD 2026 (Pacific-Asia Conference on Knowledge Discovery and Data Mining), 12 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23336v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23336v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过超单纯形投影实现可微的零一损失</div>
<div class="mono" style="margin-top:8px">机器学习的最新进展强调将结构化优化组件整合到端到端的可微模型中，以实现更丰富的归纳偏置并更紧密地与特定任务目标对齐。在本文中，我们提出了一种新的可微近似零一损失的方法-长期以来被视为分类性能的金标准，但由于其非可微性，无法与基于梯度的优化兼容。我们的方法通过约束优化框架构建了一个光滑的、保持顺序的投影到n,k维超单纯形上，从而提出了一种新的操作符，称为Soft-Binary-Argmax。在推导其数学性质后，我们展示了如何高效地计算其雅可比并将其集成到二元和多分类学习系统中。实验上，我们的方法通过在输出logits上施加几何一致性约束，在大规模训练中实现了显著的泛化改进，从而缩小了传统上观察到的大规模训练性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of integrating the zero-one loss into differentiable models by proposing a smooth approximation called Soft-Binary-Argmax, which is derived through a constrained optimization framework. The method projects onto the n,k-dimensional hypersimplex, enabling gradient-based optimization. Empirically, the approach improves generalization in large-batch training by imposing geometric consistency constraints on output logits, thereby reducing the performance gap observed in such settings.</div>
<div class="mono" style="margin-top:8px">该研究通过提出一种光滑近似方法Soft-Binary-Argmax，解决了将零一损失整合到可微模型中的难题，该方法通过约束优化框架投影到n,k维超单纯形上，从而支持梯度优化。实验结果显示，该方法通过在输出logits上施加几何一致性约束，提高了大批次训练中的泛化能力，缩小了与小批次训练之间的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset</div>
<div class="meta-line">Authors: Dany Haddad, Dan Bareket, Joseph Chee Chang, Jay DeYoung, Jena D. Hwang, Uri Katz, Mark Polak, Sangho Suh, Harshit Surana, Aryeh Tiktinsky, Shriya Atmakuri, Jonathan Bragg, Mike D&#x27;Arcy, Sergey Feldman, Amal Hassan-Ali, Rubén Lozano, Bodhisattwa Prasad Majumder, Charles McGrady, Amanpreet Singh, Brooke Vlahos, Yoav Goldberg, Doug Downey</div>
<div class="meta-line">First: 2026-02-26T18:40:28+00:00 · Latest: 2026-02-26T18:40:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23335v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23335v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries and interaction logs from two deployed tools (a literature discovery interface and a scientific question-answering interface) within an LLM-powered retrieval-augmented generation platform. Using this dataset, we characterize query patterns, engagement behaviors, and how usage evolves with experience. We find that users submit longer and more complex queries than in traditional search, and treat the system as a collaborative research partner, delegating tasks such as drafting content and identifying research gaps. Users treat generated responses as persistent artifacts, revisiting and navigating among outputs and cited evidence in non-linear ways. With experience, users issue more targeted queries and engage more deeply with supporting citations, although keyword-style queries persist even among experienced users. We release the anonymized dataset and analysis with a new query intent taxonomy to inform future designs of real-world AI research assistants and to support realistic evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解AI驱动的科学研究工具的使用与参与：Asta交互数据集</div>
<div class="mono" style="margin-top:8px">AI驱动的科学研究工具正迅速融入研究工作流程，但该领域缺乏一个清晰的视角来了解研究人员在实际环境中如何使用这些系统。我们介绍了并分析了Asta交互数据集，这是一个包含超过200,000个用户查询和交互日志的大规模资源，来自两个部署工具（文献发现界面和科学问题解答界面）在一个基于LLM的检索增强生成平台上。利用该数据集，我们描述了查询模式、参与行为以及使用随经验如何演变。我们发现，用户提交的查询比传统搜索更长、更复杂，并将系统视为协作研究伙伴，分配诸如起草内容和识别研究空白等任务。用户将生成的响应视为持久化的成果，以非线性方式反复访问和导航输出以及引用的证据。随着经验的积累，用户提出更针对性的查询，并更深入地参与支持引用，尽管经验丰富的用户仍然使用关键词式的查询。我们发布了匿名数据集和分析，以及一个新的查询意图分类法，以指导未来现实世界AI研究助手的设计，并支持现实的评估。</div>
</details>
</div>
<div class="card">
<div class="title">Utilizing LLMs for Industrial Process Automation</div>
<div class="meta-line">Authors: Salim Fares</div>
<div class="meta-line">First: 2026-02-26T18:38:00+00:00 · Latest: 2026-02-26T18:38:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23331v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23331v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大语言模型进行工业过程自动化</div>
<div class="mono" style="margin-top:8px">近年来，越来越多的研究论文探讨了使用大语言模型（LLMs）进行软件工程的最佳实践。然而，大多数研究工作集中在广泛使用的通用编程语言（如Python）上，因为这些语言的训练数据使用广泛。工业过程自动化领域中使用高度专业化语言的软件应用，这些语言通常仅在专有环境中使用，其潜力尚未得到充分探索。本研究旨在利用和整合LLMs于工业开发过程中，解决实际编程任务（例如，生成机器人手臂的运动程序），并加速制造系统的开发周期。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to explore the application of Large Language Models (LLMs) in industrial process automation, where highly-specialized languages are commonly used. The study utilizes LLMs to generate programming tasks such as creating movement routines for robotic arms, thereby accelerating the development of manufacturing systems. Key findings show that LLMs can effectively handle specialized industrial languages and improve development efficiency.</div>
<div class="mono" style="margin-top:8px">研究旨在探索大型语言模型（LLMs）在工业过程自动化中的应用，重点关注在专有环境中使用的高度专业化语言。研究使用LLMs生成编程任务，如机器人手臂的运动规程，以加速制造系统的开发周期。主要发现表明，LLMs能够有效处理专业化语言并高效完成实际编程任务。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks</div>
<div class="meta-line">Authors: Kunihiro Miyazaki, Takanobu Kawahara, Stephen Roberts, Stefan Zohren</div>
<div class="meta-line">First: 2026-02-26T18:37:36+00:00 · Latest: 2026-02-26T18:37:36+00:00</div>
<div class="meta-line">Comments: 14 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23330v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23330v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system&#x27;s output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>朝向专家投资团队：细粒度交易任务的多智能体LLM系统</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的进步加速了自主金融交易系统的开发。虽然主流方法模仿分析师和管理者角色部署多智能体系统，但它们往往依赖于抽象指令，忽视了实际工作流程的复杂性，这可能导致推理性能下降和决策透明度降低。因此，我们提出了一种多智能体LLM交易框架，明确将投资分析细分为细粒度任务，而不是提供粗粒度指令。我们使用包含股价、财务报表、新闻和宏观经济信息的日本股票数据，在受控泄漏的回测环境中评估了所提出的框架。实验结果表明，细粒度任务分解显著提高了风险调整后的回报率，优于传统的粗粒度设计。更重要的是，对中间智能体输出的进一步分析表明，分析输出与下游决策偏好的对齐是系统性能的关键驱动因素。此外，我们进行了标准投资组合优化，利用与股票指数低相关性和每个系统输出的方差。这种方法实现了更好的性能。这些发现为在实际应用中将LLM代理应用于交易系统时设计智能体结构和任务配置做出了贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes a multi-agent LLM trading framework that decomposes investment analysis into fine-grained tasks to improve risk-adjusted returns. The framework is evaluated using Japanese stock data, showing significant improvements over conventional coarse-grained designs. The analysis of intermediate agent outputs indicates that alignment between analytical outputs and decision preferences is crucial for system performance. The approach also achieves superior performance through standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system&#x27;s output.</div>
<div class="mono" style="margin-top:8px">论文提出了一种将投资分析细分为具体任务的多智能体LLM交易框架，以提高性能，优于传统的粗粒度设计。该框架使用日本股票数据进行评估，发现细粒度任务分解显著提高了风险调整后的回报率。中间输出的分析表明，分析输出与决策偏好之间的对齐是系统性能的关键。该方法还通过利用与股票指数的低相关性和每个系统输出的方差实现了更优的资产组合优化结果。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Novice Uplift on Dual-Use, In Silico Biology Tasks</div>
<div class="meta-line">Authors: Chen Bo Calvin Zhang, Christina Q. Knight, Nicholas Kruus, Jason Hausenloy, Pedro Medeiros, Nathaniel Li, Aiden Kim, Yury Orlovskiy, Coleman Breen, Bryce Cai, Jasper Götting, Andrew Bo Liu, Samira Nedungadi, Paula Rodriguez, Yannis Yiming He, Mohamed Shaaban, Zifan Wang, Seth Donoughe, Julian Michael</div>
<div class="meta-line">First: 2026-02-26T18:37:23+00:00 · Latest: 2026-02-26T18:37:23+00:00</div>
<div class="meta-line">Comments: 59 pages, 33 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23329v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23329v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM初学者在双重用途和计算生物学任务中的提升</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在生物学基准测试中的表现越来越好，但尚不清楚它们是否提升了初学者的表现，即是否使人类能够比仅使用互联网资源时表现更好。这种不确定性是理解科学加速和双重用途风险的关键。我们进行了一个多模型、多基准的人类提升研究，比较了有LLM访问权限的初学者和仅有互联网访问权限的初学者在八个与生物安全相关的任务集上的表现。参与者在复杂问题上工作，有充足的时间（最复杂的任务最多13小时）。我们发现，LLM访问提供了显著的提升：有LLM的初学者比对照组准确度高4.16倍（95% CI [2.63, 6.87]）。在四个有专家基线的基准测试中（仅有互联网资源），有LLM的初学者在三个基准测试中表现优于专家。令人惊讶的是，独立的LLM往往超过了LLM辅助的初学者，表明用户没有从LLM中获得最强的贡献。大多数参与者（89.6%）报告称，尽管有保护措施，获取与双重用途相关的信息并不困难。总体而言，LLM显著提升了初学者在以前仅由训练有素的从业者完成的生物学任务上的表现，强调了需要在传统基准测试的同时进行持续的互动提升评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study evaluates the impact of large language models (LLMs) on novice users&#x27; performance in biology tasks, comparing LLM-assisted novices with those using only internet resources. Participants were given ample time to work on complex problems across eight biosecurity-relevant task sets. The results showed that LLM access significantly improved novice accuracy, with LLM-assisted novices being 4.16 times more accurate than those without LLMs. Notably, LLMs often outperformed LLM-assisted novices and even some experts, suggesting that users may not be fully leveraging the LLMs&#x27; capabilities. The study highlights the need for ongoing evaluations of LLMs&#x27; impact on novice users in biological tasks.</div>
<div class="mono" style="margin-top:8px">本研究评估了大型语言模型（LLMs）对初学者在生物学任务中表现的影响，将使用LLMs的初学者与仅使用互联网资源的初学者进行了比较。参与者被给予充足的时间来解决八个与生物安全相关的任务集中的复杂问题。结果显示，LLM访问显著提高了初学者的准确性，LLM辅助的初学者比没有LLM的初学者准确度高4.16倍。值得注意的是，LLMs往往超过了LLM辅助的初学者，甚至一些专家，表明用户可能没有充分利用LLMs的能力。研究强调了在传统基准测试之外，持续评估LLMs对初学者在生物学任务中的影响的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">DropVLA: An Action-Level Backdoor Attack on Vision--Language--Action Models</div>
<div class="meta-line">Authors: Zonghuan Xu, Xiang Zheng, Xingjun Ma, Yu-Gang Jiang</div>
<div class="meta-line">First: 2025-10-13T02:45:48+00:00 · Latest: 2026-02-26T18:32:27+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 tables, 3 figures. Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10932v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10932v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models map multimodal perception and language instructions to executable robot actions, making them particularly vulnerable to behavioral backdoor manipulation: a hidden trigger introduced during training can induce unintended physical actions while nominal task performance remains intact. Prior work on VLA backdoors primarily studies untargeted attacks or task-level hijacking, leaving fine-grained control over individual actions largely unexplored. In this work, we present DropVLA, an action-level backdoor attack that forces a reusable action primitive (e.g., open_gripper) to execute at attacker-chosen decision points under a realistic pipeline-black-box setting with limited data-poisoning access, using a window-consistent relabeling scheme for chunked fine-tuning. On OpenVLA-7B evaluated with LIBERO, vision-only poisoning achieves 98.67%-99.83% attack success rate (ASR) with only 0.31% poisoned episodes while preserving 98.50%-99.17% clean-task retention, and successfully triggers the targeted action within 25 control steps at 500 Hz (0.05 s). Text-only triggers are unstable at low poisoning budgets, and combining text with vision provides no consistent ASR improvement over vision-only attacks. The backdoor remains robust to moderate trigger variations and transfers across evaluation suites (96.27%, 99.09%), whereas text-only largely fails (0.72%). We further validate physical-world feasibility on a 7-DoF Franka arm with pi0-fast, demonstrating non-trivial attack efficacy under camera-relative motion that induces image-plane trigger drift. These results reveal that VLA models can be covertly steered at the granularity of safety-critical actions with minimal poisoning and without observable degradation of nominal performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DropVLA：视觉-语言-行动模型中的行动级后门攻击</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型将多模态感知和语言指令映射为可执行的机器人动作，使其特别容易受到行为后门操纵：在训练期间引入的隐藏触发器可以在不影响名义任务性能的情况下诱导意外的物理动作。先前对VLA后门的研究主要集中在无目标攻击或任务级劫持上，而对个体动作的精细控制尚未得到充分探索。在本研究中，我们提出了DropVLA，这是一种行动级后门攻击，能够在有限的数据污染访问和现实的管道黑盒设置下，通过窗口一致的重新标记方案进行分块微调，迫使可重用的动作原语（例如，open_gripper）在攻击者选择的决策点执行。在使用LIBERO评估的OpenVLA-7B中，仅通过视觉污染即可实现98.67%-99.83%的攻击成功率（ASR），污染的剧集比例仅为0.31%，同时保持98.50%-99.17%的任务清洁保留率，并在25个控制步骤内（500 Hz，0.05秒）成功触发目标动作。仅文本触发在低污染预算下不稳定，结合文本与视觉并不能在视觉污染攻击上提供一致的ASR改进。后门对触发器的适度变化具有鲁棒性，并且可以在评估套件之间转移（96.27%，99.09%），而仅文本则大多失败（0.72%）。我们还在7自由度的Franka手臂上通过pi0-fast验证了物理世界的可行性，展示了在相机相对运动下诱导图像平面触发漂移的非平凡攻击效果。这些结果表明，VLA模型可以在最小的污染和无明显名义性能退化的情况下，被隐蔽地引导至关键安全动作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action (VLA) models map multimodal perception and language instructions to executable robot actions, making them particularly vulnerable to behavioral backdoor manipulation: a hidden trigger introduced during training can induce unintended physical actions while nominal task performance remains intact.</div>
</details>
</div>
<div class="card">
<div class="title">ParamMem: Augmenting Language Agents with Parametric Reflective Memory</div>
<div class="meta-line">Authors: Tianjun Yao, Yongqiang Chen, Yujia Zheng, Pan Li, Zhiqiang Shen, Kun Zhang</div>
<div class="meta-line">First: 2026-02-26T18:28:04+00:00 · Latest: 2026-02-26T18:28:04+00:00</div>
<div class="meta-line">Comments: 20 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23320v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23320v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ParamMem：通过参数化反思记忆增强语言代理</div>
<div class="mono" style="margin-top:8px">自我反思使语言代理能够迭代地改进解决方案，但往往会产生重复的输出，限制了推理性能。最近的研究试图通过各种方法解决这一限制，其中增加反思多样性显示出前景。我们的实证分析揭示了反思多样性和任务成功率之间存在强烈的正相关关系，进一步突出了多样化反思信号的必要性。我们引入了ParamMem，这是一种参数化记忆模块，将跨样本的反思模式编码到模型参数中，通过温度控制采样实现多样化的反思生成。在此模块的基础上，我们提出了ParamAgent，这是一种结合参数化记忆和情景记忆及跨样本记忆的基于反思的代理框架。在代码生成、数学推理和多跳问答等广泛实验中，ParamAgent 显示出对最先进的基线方法的一致改进。进一步的分析表明，ParamMem 具有样本效率高、能够在不同模型规模之间实现弱到强的迁移，并支持无需依赖更强的外部模型即可实现自我改进，突显了ParamMem作为增强语言代理的有效组件的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance language agents&#x27; reasoning capabilities by increasing reflective diversity, which often suffers from repetitive outputs. ParamMem, a parametric memory module, is introduced to encode cross-sample reflection patterns into model parameters, allowing for diverse reflection generation. Experiments on code generation, mathematical reasoning, and multi-hop question answering show consistent improvements over existing methods, indicating ParamMem&#x27;s effectiveness in enhancing language agents. The module is sample-efficient and supports weak-to-strong transfer across model scales, enabling self-improvement without relying on stronger external models.</div>
<div class="mono" style="margin-top:8px">研究旨在通过增强语言代理的自我反思能力来提高其推理性能。引入了ParamMem参数记忆模块，将多样化的反思模式编码到模型参数中，通过温度控制采样生成多样化的反思信号。在代码生成、数学推理和多跳问答等实验中，ParamMem表现出一致的改进效果，表明该模块具有样本效率高、支持不同模型规模之间的迁移学习，并且无需依赖更强的外部模型即可实现自我改进。</div>
</details>
</div>
<div class="card">
<div class="title">LinGuinE: Longitudinal Guidance Estimation for Volumetric Tumour Segmentation</div>
<div class="meta-line">Authors: Nadine Garibli, Mayank Patwari, Bence Csiba, Yi Wei, Kostantinos Sidiropoulos</div>
<div class="meta-line">First: 2025-06-06T13:52:33+00:00 · Latest: 2026-02-26T18:27:23+00:00</div>
<div class="meta-line">Comments: 10 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06092v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.06092v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Longitudinal volumetric tumour segmentation is critical for radiotherapy planning and response assessment, yet this problem is underexplored and most methods produce single-timepoint semantic masks, lack lesion correspondence, and offer limited radiologist control. We introduce LinGuinE (Longitudinal Guidance Estimation), a PyTorch framework that combines image registration and guided segmentation to deliver lesion-level tracking and volumetric masks across all scans in a longitudinal study from a single radiologist prompt. LinGuinE is temporally direction agnostic, requires no training on longitudinal data, and allows any registration and semi-automatic segmentation algorithm to be repurposed for the task. We evaluate various combinations of registration and segmentation algorithms within the framework. LinGuinE achieves state-of-the-art segmentation and tracking performance across four datasets with a total of 456 longitudinal studies. Tumour segmentation performance shows minimal degradation with increasing temporal separation. We conduct ablation studies to determine the impact of autoregression, pathology specific finetuning, and the use of real radiologist prompts. We release our code and substantial public benchmarking for longitudinal segmentation, facilitating future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LinGuinE: 长期肿瘤分割的纵向引导估计</div>
<div class="mono" style="margin-top:8px">纵向体积肿瘤分割对于放射治疗计划和反应评估至关重要，但这一问题尚未得到充分探索，大多数方法仅生成单时点语义掩码，缺乏病灶对应关系，且对放射科医生的控制有限。我们引入了LinGuinE（纵向引导估计），这是一种结合图像配准和引导分割的PyTorch框架，能够从单个放射科医生的提示中在纵向研究的所有扫描中提供病灶级跟踪和体积掩码。LinGuinE在时间方向上是无方向性的，无需在纵向数据上进行训练，并允许任何配准和半自动分割算法重新用于此任务。我们评估了框架内各种配准和分割算法的组合。LinGuinE在四个数据集的456个纵向研究中实现了最先进的分割和跟踪性能。肿瘤分割性能随时间间隔增加而略有下降。我们进行了消融研究以确定自回归、病理特异性微调和使用真实放射科医生提示的影响。我们发布了我们的代码和大量公共基准测试，促进未来的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LinGuinE is a PyTorch framework for longitudinal volumetric tumour segmentation that combines image registration and guided segmentation to provide lesion-level tracking across all scans in a longitudinal study. It does not require training on longitudinal data and allows the use of any registration and semi-automatic segmentation algorithm. LinGuinE achieves state-of-the-art performance across four datasets with 456 longitudinal studies, showing minimal degradation in tumour segmentation performance with increasing temporal separation.</div>
<div class="mono" style="margin-top:8px">LinGuinE 是一个结合图像注册和引导分割的 PyTorch 框架，用于纵向体素肿瘤分割，提供纵向研究中所有扫描的病变级跟踪和体素掩码。它不需要在纵向数据上进行训练，并允许任何注册和半自动分割算法重新用于此任务。LinGuinE 在四个数据集的 456 个纵向研究中实现了最先进的性能，随着时间间隔的增加，肿瘤分割性能的下降幅度很小。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction</div>
<div class="meta-line">Authors: Rafael R. Baptista, André de Lima Salgado, Ricardo V. Godoy, Marcelo Becker, Thiago Boaventura, Gustavo J. G. Lahr</div>
<div class="meta-line">First: 2026-02-26T18:20:26+00:00 · Latest: 2026-02-26T18:20:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23312v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23312v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment. Small language models (SLMs) offer a potential alternative, but their effectiveness for role classification in HRI has not been systematically evaluated. In this paper, we present a benchmark of SLMs for leader-follower communication, introducing a novel dataset derived from a published database and augmented with synthetic samples to capture interaction-specific dynamics. We investigate two adaptation strategies: prompt engineering and fine-tuning, studied under zero-shot and one-shot interaction modes, compared with an untrained baseline. Experiments with Qwen2.5-0.5B reveal that zero-shot fine-tuning achieves robust classification performance (86.66% accuracy) while maintaining low latency (22.2 ms per sample), significantly outperforming baseline and prompt-engineered approaches. However, results also indicate a performance degradation in one-shot modes, where increased context length challenges the model&#x27;s architectural capacity. These findings demonstrate that fine-tuned SLMs provide an effective solution for direct role assignment, while highlighting critical trade-offs between dialogue complexity and classification reliability on the edge.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper evaluates the effectiveness of small language models (SLMs) for leader-follower interaction in human-robot interaction (HRI). It introduces a benchmark using a novel dataset and investigates zero-shot and one-shot adaptation strategies, including prompt engineering and fine-tuning. Experiments with Qwen2.5-0.5B show that zero-shot fine-tuning achieves high accuracy (86.66%) and low latency (22.2 ms per sample), outperforming baseline and prompt-engineered approaches, but performance drops in one-shot modes due to increased context length challenges.</div>
<div class="mono" style="margin-top:8px">本文评估了小语言模型（SLMs）在人类-机器人交互（HRI）中的领导者-跟随者互动效果。研究引入了一个新的基准，并探讨了两种适应策略：提示工程和微调。实验表明，零样本微调在Qwen2.5-0.5B上实现了高准确率（86.66%）和低延迟（每样本22.2毫秒），优于基线和提示工程方法。然而，在单样本模式下，由于上下文长度增加导致性能下降，这突显了对话复杂性和分类可靠性之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating the Diversity and Quality of LLM Generated Content</div>
<div class="meta-line">Authors: Alexander Shypula, Shuo Li, Botong Zhang, Vishakh Padmakumar, Kayo Yin, Osbert Bastani</div>
<div class="meta-line">First: 2025-04-16T23:02:23+00:00 · Latest: 2026-02-26T18:17:44+00:00</div>
<div class="meta-line">Comments: Published at COLM 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.12522v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.12522v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work suggests that preference-tuning techniques -- such as Reinforcement Learning from Human Feedback (RLHF) methods like PPO and GRPO, as well as alternatives like DPO -- reduce diversity, creating a dilemma given that these models are widely deployed in applications requiring varied outputs. We argue that diversity without consideration of quality has limited practical value. To address this issue, we introduce a framework for measuring effective semantic diversity -- diversity among outputs that meet quality thresholds -- which better reflects the practical utility of large language models (LLMs). Using open-ended tasks that require no human intervention, we find counterintuitive results: when using diversity metrics that do not explicitly consider quality, preference-tuned models -- particularly those trained via RL -- often produce outputs with lower diversity; however, these same preference-tuned models generate greater effective semantic diversity than supervised fine-tuned (SFT) or base models. Our analysis further shows another trend: while larger models may exhibit greater effective semantic diversity than smaller models, the smaller models are consistently more parameter-efficient at producing unique content within a fixed sampling budget. These findings have practical implications for applications that require diverse yet high-quality outputs, from creative assistance to synthetic data generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估LLM生成内容的多样性和质量</div>
<div class="mono" style="margin-top:8px">近期研究表明，偏好调优技术——如基于人类反馈强化学习（RLHF）方法（如PPO和GRPO），以及替代方法DPO——降低了多样性，这给这些模型在需要多样化输出的应用中广泛应用带来了困境。我们认为，不考虑质量的多样性在实际应用中价值有限。为解决这一问题，我们提出了一种衡量有效语义多样性的框架——衡量满足质量标准的输出之间的多样性——这更好地反映了大型语言模型（LLM）的实际效用。通过不需要人类干预的开放任务，我们发现了一些反直觉的结果：当使用不考虑质量的多样性度量时，偏好调优模型——尤其是通过RL训练的模型——往往生成的输出多样性较低；然而，这些偏好调优模型生成的有效语义多样性却大于监督微调（SFT）或基础模型。我们的分析还显示了另一种趋势：虽然较大的模型可能在固定采样预算内生成更独特的内容方面表现出更大的有效语义多样性，但较小的模型在生成独特内容方面始终更具有参数效率。这些发现对需要多样化且高质量输出的应用具有实际意义，从创意辅助到合成数据生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study evaluates the diversity and quality of content generated by large language models (LLMs) and introduces a framework for measuring effective semantic diversity, which considers both diversity and quality. Using open-ended tasks, the research finds that preference-tuned models, especially those trained via reinforcement learning, produce lower diversity when not explicitly considering quality but generate higher effective semantic diversity compared to supervised fine-tuned or base models. Additionally, smaller models are more parameter-efficient in producing unique content within a fixed budget.</div>
<div class="mono" style="margin-top:8px">该研究评估了大型语言模型（LLMs）生成内容的多样性和质量，并引入了一个同时考虑多样性和质量的有效语义多样性测量框架。使用开放任务，研究发现，尤其是通过强化学习训练的偏好调整模型，在不考虑质量的情况下生成的多样性较低，但与监督微调或基础模型相比，生成的有效语义多样性更高。此外，较小的模型在固定采样预算内生成独特内容方面更具参数效率。</div>
</details>
</div>
<div class="card">
<div class="title">Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training</div>
<div class="meta-line">Authors: Anas Barakat, Souradip Chakraborty, Khushbu Pahwa, Amrit Singh Bedi</div>
<div class="meta-line">First: 2026-02-24T18:43:08+00:00 · Latest: 2026-02-26T18:11:36+00:00</div>
<div class="meta-line">Comments: updated related work discussion</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21189v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.21189v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为什么 Pass@k 优化会降低 Pass@1：LLM 训练后提示干扰的影响</div>
<div class="mono" style="margin-top:8px">Pass@k 是用于可验证的大语言模型任务（包括数学推理、代码生成和简短答案推理）的广泛使用的性能指标。它定义为如果 $k$ 个独立采样的解决方案中有任何一个通过验证器则视为成功。这种多样本推理指标促使了推理感知微调方法的发展，这些方法直接优化 Pass@k。然而，先前的工作报告了这种方法下的一个反复出现的权衡：Pass@k 提高而 Pass@1 下降。这种权衡在实践中非常重要，因为 Pass@1 往往由于延迟和成本预算、验证器覆盖率不完善以及需要可靠的单次尝试后备而成为一项硬性操作约束。我们研究了这种权衡的起源，并提供了 Pass@k 政策优化如何通过梯度冲突导致 Pass@1 下降的理论表征，这种梯度冲突是由提示干扰引起的。我们展示了 Pass@k 政策梯度可以与 Pass@1 梯度冲突，因为 Pass@k 优化隐式地将提示重新加权为低成功率提示；当这些提示我们称之为负干扰时，它们的加权可以旋转 Pass@k 更新方向远离 Pass@1 方向。我们通过可验证的数学推理任务的大语言模型实验说明了我们的理论发现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates why optimizing for Pass@k can degrade Pass@1 in large language models, focusing on prompt interference. It shows that Pass@k optimization can conflict with Pass@1 optimization due to reweighting prompts towards low-success prompts, which can negatively impact Pass@1 performance. The study provides theoretical insights and supports its findings with experiments on verifiable mathematical reasoning tasks.</div>
<div class="mono" style="margin-top:8px">该论文研究了为什么在大型语言模型中优化Pass@k会导致Pass@1下降，重点关注提示干扰。研究表明，Pass@k优化会因重新加权低成功率的提示而与Pass@1优化发生冲突，这可能对Pass@1性能产生负面影响。该研究提供了理论见解，并通过验证数学推理任务的实验支持了其发现。</div>
</details>
</div>
<div class="card">
<div class="title">ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding</div>
<div class="meta-line">Authors: Yiran Guan, Sifan Tu, Dingkang Liang, Linghao Zhu, Jianzhong Ju, Zhenbo Luo, Jian Luan, Yuliang Liu, Xiang Bai</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-26T18:10:41+00:00 · Latest: 2026-02-26T18:10:41+00:00</div>
<div class="meta-line">Comments: Accept by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23306v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23306v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ThinkOmni：通过指导解码提升文本推理至全模态场景</div>
<div class="mono" style="margin-top:8px">全模态推理对于智能系统理解并从多种数据源中推断信息至关重要。虽然现有的全模态大型语言模型（OLLM）在感知多种模态方面表现出色，但它们缺乏近期大型推理模型（LRM）的复杂推理能力。然而，通过额外训练来增强OLLM的推理能力面临着重大挑战，包括高质量数据的需求、任务特定的适应以及巨大的计算成本。为了解决这些限制，我们提出了ThinkOmni，这是一种无需训练和数据的框架，将文本推理提升至全模态场景。ThinkOmni引入了两个关键组件：1）LRM-as-a-Guide，利用现成的LRM来指导OLLM的解码过程；2）逐步对比缩放，无需手动超参数调整即可适应性平衡感知和推理信号。在六个跨模态推理基准上的实验表明，ThinkOmni始终能够提供性能改进，主要结果在MathVista上达到70.2，在MMAU上达到75.5。总体而言，ThinkOmni提供了一种灵活且通用的全模态推理解决方案，并为推理能力的泛化和应用提供了新的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ThinkOmni is a training-free and data-free framework that enhances the reasoning ability of omni-modal large language models (OLLMs) by leveraging off-the-shelf large reasoning models (LRMs) for guidance during the decoding process and using Stepwise Contrastive Scaling to balance perception and reasoning signals. Experiments on six multi-modal reasoning benchmarks show consistent performance improvements, with ThinkOmni achieving 70.2 on MathVista and 75.5 on MMAU.</div>
<div class="mono" style="margin-top:8px">ThinkOmni 是一个无需训练和数据的框架，通过利用现成的大型推理模型（LRMs）进行指导解码和自适应平衡感知与推理信号，来增强全模态大型语言模型（OLLMs）的推理能力。在六个多模态推理基准上的实验显示了一致的性能提升，ThinkOmni 在 MathVista 达到 70.2，在 MMAU 达到 75.5。</div>
</details>
</div>
<div class="card">
<div class="title">A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations</div>
<div class="meta-line">Authors: Soumya Dutta, Smruthi Balaji, Sriram Ganapathy</div>
<div class="meta-line">First: 2026-02-26T18:08:40+00:00 · Latest: 2026-02-26T18:08:40+00:00</div>
<div class="meta-line">Comments: Accepted to Elsevier Computer Speech and Language. 30 pages, 9 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23300v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23300v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对话中多模态情感识别的专家混合模型</div>
<div class="mono" style="margin-top:8px">对话中的情感识别（ERC）提出了独特的挑战，需要模型捕捉多轮对话的时间流程并有效整合多种模态的线索。我们提出了Mixture of Speech-Text Experts for Recognition of Emotions（MiSTER-E），这是一种模块化的专家混合（MoE）框架，旨在解耦ERC中的两个核心挑战：模态特定的上下文建模和多模态信息融合。MiSTER-E 利用针对语音和文本均进行了微调的大语言模型（LLMs）提供丰富的语句级嵌入，然后通过卷积循环上下文建模层进行增强。系统通过一个学习到的门控机制整合来自三个专家（仅语音、仅文本和跨模态）的预测。为了进一步鼓励模态间的一致性和对齐，我们引入了配对语音-文本表示之间的监督对比损失以及基于KL散度的专家预测正则化。重要的是，MiSTER-E 在任何阶段都不依赖说话人身份。在三个基准数据集IEMOCAP、MELD和MOSI上的实验表明，我们的提议分别实现了70.9%、69.5%和87.9%的加权F1分数，优于几种基线的语音-文本ERC系统。我们还提供了各种消融实验以突出所提出方法的贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenges of Emotion Recognition in Conversations (ERC) by proposing MiSTER-E, a modular Mixture-of-Experts framework that decouples modality-specific context modeling and multimodal information fusion. It uses large language models fine-tuned for speech and text to generate rich embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts (speech-only, text-only, and cross-modal) using a gating mechanism and includes a supervised contrastive loss and KL-divergence regularization to ensure modality consistency. Experiments on IEMOCAP, MELD, and MOSI show that MiSTER-E outperforms baseline systems with weighted F1-scores of 70.9%, 69.5%, and 87.9%, respectively.</div>
<div class="mono" style="margin-top:8px">论文提出了一种模块化的Mixture-of-Experts框架MiSTER-E，以解决对话中的情感识别挑战，该框架将模态特定上下文建模和多模态信息融合分离。它使用针对语音和文本进行微调的大语言模型生成丰富的短语级嵌入，然后通过卷积-循环上下文建模层进行增强。该系统通过一个学习到的门控机制整合了三种专家的预测：仅语音、仅文本和跨模态。实验结果表明，MiSTER-E在IEMOCAP、MELD和MOSI数据集上的加权F1分数分别为70.9%、69.5%和87.9%，优于基线系统。</div>
</details>
</div>
<div class="card">
<div class="title">PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM</div>
<div class="meta-line">Authors: Yiqing Wang, Chunming He, Ming-Chen Lu, Mercy Pawar, Leslie Niziol, Maria Woodward, Sina Farsiu</div>
<div class="meta-line">First: 2026-02-26T18:07:52+00:00 · Latest: 2026-02-26T18:07:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23297v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23297v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRIMA：通过LLM进行风险整合图像-元数据对齐的预训练以实现医学诊断</div>
<div class="mono" style="margin-top:8px">医学诊断需要有效地综合视觉表现和临床元数据。然而，现有方法往往将元数据视为孤立的标签，未能利用嵌入在临床描述中的丰富语义知识。我们提出了PRIMA（风险整合图像-元数据对齐的预训练），这是一种将领域特定知识整合到多模态表示学习中的框架。我们首先通过检索增强生成（RAG）构建专家级的风险-疾病关联语料库，以精炼Clinical ModernBERT，将诊断先验嵌入到文本编码器中。为了弥合模态差距，我们引入了一种双编码器预训练策略，利用DINOv3和我们精炼的BERT，并通过一系列互补的损失函数进行优化。这些损失函数旨在捕捉多粒度语义对齐，并通过软标签处理临床关联的模糊性。最后，我们利用Qwen-3融合这些对齐的特征，以实现精确的疾病分类。广泛的实验表明，PRIMA有效地协调了像素级特征与抽象的临床专业知识，显著优于其他最先进的方法。值得注意的是，我们的框架在无需大量数据收集或耗尽计算资源的情况下实现了卓越的鲁棒性。我们的代码将在接受后公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PRIMA is a framework that integrates risk-disease correlations into multi-modal representation learning for medical diagnosis. It uses a dual-encoder pre-training strategy with DINOv3 and a refined BERT, optimized by four loss functions to align image and metadata. Experiments show that PRIMA outperforms existing methods in disease classification, achieving robust performance without requiring large datasets or extensive computational resources.</div>
<div class="mono" style="margin-top:8px">PRIMA 是一个框架，将领域特定知识整合到多模态表示学习中以进行医学诊断。它使用 DINOv3 和一个改进的 BERT 的双编码器预训练策略，并通过四种损失函数进行优化，以捕捉语义对齐并处理临床相关性的模糊性。PRIMA 在疾病分类中显著优于其他最先进的方法，并且在不需要大量数据或计算资源的情况下表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity</div>
<div class="meta-line">Authors: Quang-Huy Nguyen, Jiaqi Wang, Wei-Shinn Ku</div>
<div class="meta-line">First: 2026-02-26T18:07:45+00:00 · Latest: 2026-02-26T18:07:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23296v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23296v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. Conformal prediction is a widely used distribution-free UQ framework, yet its applications in heterogeneous FL settings remains underexplored. We provide FedWQ-CP, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. FedWQ-CP performs agent-server calibration in a single communication round. On each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. Each agent then transmits only its quantile threshold and calibration sample size to the server. The server simply aggregates these thresholds through a weighted average to produce a global threshold. Experimental results on seven public datasets for both classification and regression demonstrate that FedWQ-CP empirically maintains agent-wise and global coverage while producing the smallest prediction sets or intervals.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of uncertainty quantification in federated learning, particularly under dual heterogeneity. It introduces FedWQ-CP, a method that balances empirical coverage performance with efficiency by performing agent-server calibration in a single communication round. The approach computes conformity scores on calibration data and derives a local quantile threshold on each agent, which is then aggregated at the server to produce a global threshold. Experiments on seven public datasets show that FedWQ-CP maintains agent-wise and global coverage while producing the smallest prediction sets or intervals.</div>
<div class="mono" style="margin-top:8px">论文解决了联邦学习中不确定性量化的问题，特别是在数据和模型异构性并存的环境中。提出了一种名为FedWQ-CP的方法，利用校准预测在单次通信轮中对各代理进行校准。该方法在保持覆盖率的同时，通过仅传输量阈值和校准样本数量到服务器进行全局聚合，平衡了实际覆盖率和效率。实验结果表明，FedWQ-CP在七个数据集上保持了可靠的覆盖率，同时产生了最小的预测区间。</div>
</details>
</div>
<div class="card">
<div class="title">ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation</div>
<div class="meta-line">Authors: Ayush Roy, Wei-Yang Alex Lee, Rudrasis Chakraborty, Vishnu Suresh Lokhande</div>
<div class="meta-line">First: 2026-02-26T18:07:10+00:00 · Latest: 2026-02-26T18:07:10+00:00</div>
<div class="meta-line">Comments: CVPE 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23295v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23295v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. This formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. Empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of FID, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing ManifoldGD as the first geometry-aware training-free data distillation framework.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ManifoldGD is a training-free diffusion-based framework that enhances dataset distillation by integrating manifold consistent guidance at each denoising step. It uses hierarchical clustering of VAE latent features to compute instance prototype centroids (IPCs) at multiple scales, ensuring both coarse semantic modes and fine intra-class variability are captured. This method projects the mode-alignment vector onto the local tangent space of the estimated latent manifold, maintaining semantic consistency and improving representativeness and image fidelity. Experiments show consistent improvements over existing training-free and training-based methods in terms of FID, l2 distance, and classification accuracy.</div>
<div class="mono" style="margin-top:8px">ManifoldGD 是一种训练-free 的扩散模型框架，通过在每个去噪步骤中整合流形一致的指导来增强数据集蒸馏。它使用 VAE 潜在特征的分层聚类来创建多尺度的实例原型中心 (IPCs) 聚合集，用于定义每个去噪步骤的潜在流形。这种方法确保生成轨迹保持流形一致性并保留语义一致性，从而提高表示性、多样性和图像保真度。实验结果显示，在 FID、l2 距离和分类准确性方面，ManifoldGD 在现有训练-free 和训练-based 方法上具有持续的改进。</div>
</details>
</div>
<div class="card">
<div class="title">PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions</div>
<div class="meta-line">Authors: Amith Ananthram, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Robert Stein, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-21T20:30:20+00:00 · Latest: 2026-02-26T18:05:42+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026. 26 pages, 9 figures. Metric/benchmark available at https://github.com/amith-ananthram/posh</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.19060v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.19060v3">PDF</a> · <a href="https://github.com/amith-ananthram/posh">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts require sensitivity to attribute and relation attachments and scores that localize errors to particular text spans. In this work, we introduce PoSh, a metric for detailed image description that uses scene graphs as structured rubrics to guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained errors (e.g. mistakes in compositional understanding). PoSh is replicable, interpretable and a better proxy for human raters than existing metrics (including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new dataset, DOCENT. This novel benchmark contains artwork, paired with expert-written references, and model-generated descriptions, augmented with granular and coarse judgments of their quality from art history students. Thus, DOCENT enables evaluating both detailed image description metrics and detailed image description itself in a challenging new domain. We show that PoSh achieves stronger correlations (+0.05 Spearman $ρ$) with the human judgments in DOCENT than the best open-weight alternatives, is robust to image type (using CapArena, an existing dataset of web imagery) and is a capable reward function, outperforming standard supervised fine-tuning. Then, using PoSh, we characterize the performance of open and closed models in describing the paintings, sketches and statues in DOCENT and find that foundation models struggle to achieve full, error-free coverage of images with rich scene dynamics, establishing a demanding new task to gauge VLM progress. Through both PoSh and DOCENT, we hope to enable advances in important areas such as assistive text generation.</div></details>
</div>
<div class="card">
<div class="title">Abstracted Gaussian Prototypes for True One-Shot Concept Learning</div>
<div class="meta-line">Authors: Chelsea Zou, Kenneth J. Kurtz</div>
<div class="meta-line">First: 2024-08-30T12:50:15+00:00 · Latest: 2026-02-26T18:03:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2408.17251v2">Abs</a> · <a href="https://arxiv.org/pdf/2408.17251v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a cluster-based generative image segmentation framework to encode higher-level representations of visual concepts based on one-shot learning inspired by the Omniglot Challenge. The inferred parameters of each component of a Gaussian Mixture Model (GMM) represent a distinct topological subpart of a visual concept. Sampling new data from these parameters generates augmented subparts to build a more robust prototype for each concept, i.e., the Abstracted Gaussian Prototype (AGP). This framework addresses one-shot classification tasks using a cognitively-inspired similarity metric and addresses one-shot generative tasks through a novel AGP-VAE pipeline employing variational autoencoders (VAEs) to generate new class variants. Results from human judges reveal that the generative pipeline produces novel examples and classes of visual concepts that are broadly indistinguishable from those made by humans. The proposed framework leads to impressive, but not state-of-the-art, classification accuracy; thus, the contribution is two-fold: 1) the system is low in theoretical and computational complexity yet achieves the standard of &#x27;true&#x27; one-shot learning by operating in a fully standalone manner unlike existing approaches that draw heavily on pre-training or knowledge engineering; and 2) in contrast with existing neural network approaches, the AGP approach addresses the importance of broad task capability emphasized in the Omniglot challenge (successful performance on classification and generative tasks). These two points are critical in advancing our understanding of how learning and reasoning systems can produce viable, robust, and flexible concepts based on literally no more than a single example.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>抽象高斯原型用于真正的单次概念学习</div>
<div class="mono" style="margin-top:8px">我们提出了一种基于聚类生成的图像分割框架，以基于Omniglot挑战启发的一次性学习来编码视觉概念的高层表示。每个高斯混合模型（GMM）组件的推断参数代表视觉概念的一个独特的拓扑子部分。从这些参数中采样新的数据生成增强的子部分，以构建每个概念的更稳健的原型，即抽象高斯原型（AGP）。该框架使用认知启发的相似度度量解决了一次性分类任务，并通过一种新颖的AGP-VAE流水线利用变分自编码器（VAEs）生成新的类别变体来解决一次性生成任务。人类评委的结果表明，生成流水线生成的新型示例和视觉概念类别在广泛上与人类生成的无异。所提框架在分类准确性上取得了令人印象深刻但尚未达到最新技术水平的结果；因此，贡献有两个方面：1）该系统在理论和计算复杂性上较低，通过完全独立的方式实现真正的单次学习，不同于现有依赖预训练或知识工程的方法；2）与现有的神经网络方法不同，AGP方法解决了Omniglot挑战中强调的广泛任务能力的重要性（在分类和生成任务上均表现出色）。这两点对于推进我们对如何基于单个示例几乎没有任何其他信息来生成可行、稳健和灵活的概念的理解至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a cluster-based generative image segmentation framework for one-shot concept learning, using Gaussian Mixture Models to represent visual concepts. The framework generates robust prototypes through sampling from inferred parameters, and employs a novel AGP-VAE pipeline for generative tasks. Human judges found the generated examples to be indistinguishable from human-made ones. The system achieves &#x27;true&#x27; one-shot learning with low complexity and performs well on both classification and generative tasks, contributing to a better understanding of concept learning from minimal data.</div>
<div class="mono" style="margin-top:8px">该论文提出了一种基于聚类的生成图像分割框架，用于一-shot 概念学习，灵感来源于 Omniglot 挑战。它使用高斯混合模型来表示视觉概念，并生成增强的子部分以创建更 robust 的原型。该框架采用新颖的 AGP-VAE 管道生成新类变体，并在生成任务中达到了人类水平的性能。虽然分类准确率不是最先进的，但该系统复杂度低且独立运行，是真正的 one-shot 学习的重要贡献。</div>
</details>
</div>
<div class="card">
<div class="title">PGVMS: A Prompt-Guided Unified Framework for Virtual Multiplex IHC Staining with Pathological Semantic Learning</div>
<div class="meta-line">Authors: Fuqiang Chen, Ranran Zhang, Wanming Hu, Deboch Eyob Abera, Yue Peng, Boyun Zheng, Yiwen Sun, Jing Cai, Wenjian Qin</div>
<div class="meta-line">Venue: IEEE Transactions on Medical Imaging, 2026</div>
<div class="meta-line">First: 2026-02-26T18:03:24+00:00 · Latest: 2026-02-26T18:03:24+00:00</div>
<div class="meta-line">Comments: Accepted by TMI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23292v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23292v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Immunohistochemical (IHC) staining enables precise molecular profiling of protein expression, with over 200 clinically available antibody-based tests in modern pathology. However, comprehensive IHC analysis is frequently limited by insufficient tissue quantities in small biopsies. Therefore, virtual multiplex staining emerges as an innovative solution to digitally transform H&amp;E images into multiple IHC representations, yet current methods still face three critical challenges: (1) inadequate semantic guidance for multi-staining, (2) inconsistent distribution of immunochemistry staining, and (3) spatial misalignment across different stain modalities. To overcome these limitations, we present a prompt-guided framework for virtual multiplex IHC staining using only uniplex training data (PGVMS). Our framework introduces three key innovations corresponding to each challenge: First, an adaptive prompt guidance mechanism employing a pathological visual language model dynamically adjusts staining prompts to resolve semantic guidance limitations (Challenge 1). Second, our protein-aware learning strategy (PALS) maintains precise protein expression patterns by direct quantification and constraint of protein distributions (Challenge 2). Third, the prototype-consistent learning strategy (PCLS) establishes cross-image semantic interaction to correct spatial misalignments (Challenge 3).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PGVMS：一种基于提示的统一框架，用于病理语义学习的虚拟多路复用IHC染色</div>
<div class="mono" style="margin-top:8px">免疫组化(IHC)染色能够精确地对蛋白质表达进行分子分析，在现代病理学中已有超过200种基于抗体的临床测试。然而，全面的IHC分析经常受限于小活检组织量不足。因此，虚拟多路复用染色作为一种创新解决方案，可以将HE图像数字化转换为多种IHC表示，但当前方法仍面临三个关键挑战：(1) 多染色的不足语义指导，(2) 免疫化学染色分布不一致，(3) 不同染色模式之间的空间错位。为克服这些限制，我们提出了一种仅使用单路训练数据的基于提示的虚拟多路复用IHC染色框架（PGVMS）。我们的框架引入了三个关键创新，分别对应每个挑战：首先，一种自适应提示引导机制，利用病理视觉语言模型动态调整染色提示，以解决语义指导不足的问题（挑战1）。其次，我们的蛋白质感知学习策略（PALS）通过直接量化和约束蛋白质分布来保持精确的蛋白质表达模式（挑战2）。第三，原型一致学习策略（PCLS）建立了跨图像语义交互，以纠正空间错位（挑战3）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PGVMS is a prompt-guided framework for virtual multiplex IHC staining that addresses three key challenges: inadequate semantic guidance, inconsistent staining distribution, and spatial misalignment. It uses an adaptive prompt guidance mechanism, a protein-aware learning strategy, and a prototype-consistent learning strategy to improve the accuracy and consistency of virtual multiplex IHC staining from uniplex training data.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出使用单染训练数据的提示引导统一框架（PGVMS）来解决虚拟多路复用IHC染色的限制。该框架引入了三个关键创新：一种自适应提示引导机制以解决语义指导问题，一种蛋白质感知学习策略以保持精确的蛋白质表达模式，以及一种原型一致学习策略以纠正空间错位。主要实验结果表明，与现有方法相比，虚拟多路复用IHC染色的准确性和一致性得到了提高。</div>
</details>
</div>
<div class="card">
<div class="title">LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction</div>
<div class="meta-line">Authors: Zhengyang Wei, Renzhi Jing, Yiyi He, Jenny Suckale</div>
<div class="meta-line">First: 2026-02-26T18:02:44+00:00 · Latest: 2026-02-26T18:02:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23290v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23290v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The accurate and automatic extraction of roads from satellite imagery is critical for applications in navigation and urban planning, significantly reducing the need for manual annotation. Many existing methods decompose this task into keypoint extraction and connectedness prediction, but often struggle to capture long-range dependencies and complex topologies. Here, we propose LineGraph2Road, a framework that improves connectedness prediction by formulating it as binary classification over edges in a constructed global but sparse Euclidean graph, where nodes are keypoints extracted from segmentation masks and edges connect node pairs within a predefined distance threshold, representing potential road segments. To better learn structural link representation, we transform the original graph into its corresponding line graph and apply a Graph Transformer on it for connectedness prediction. This formulation overcomes the limitations of endpoint-embedding fusion on set-isomorphic links, enabling rich link representations and effective relational reasoning over the global structure. Additionally, we introduce an overpass/underpass head to resolve multi-level crossings and a coupled NMS strategy to preserve critical connections. We evaluate LineGraph2Road on three benchmarks: City-scale, SpaceNet, and Global-scale, and show that it achieves state-of-the-art results on two key metrics, TOPO-F1 and APLS. It also captures fine visual details critical for real-world deployment. We will make our code publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LineGraph2Road：基于线图的结构图推理在道路网络提取中的应用</div>
<div class="mono" style="margin-top:8px">从卫星图像中准确且自动地提取道路对于导航和城市规划应用至关重要，显著减少了手动标注的需求。许多现有方法将此任务分解为关键点提取和连通性预测，但往往难以捕捉长距离依赖性和复杂拓扑结构。在此，我们提出了一种名为LineGraph2Road的框架，通过将其形式化为在构建的全局但稀疏欧几里得图中对边进行二元分类来改进连通性预测，其中节点是从分割掩码中提取的关键点，边连接预定义距离阈值内的节点对，表示潜在的道路段。为了更好地学习结构链接表示，我们将原始图转换为其对应的线图，并在其上应用图变换器进行连通性预测。这种形式克服了端点嵌入融合在集同构链接上的局限性，使链接表示更加丰富，并且能够在全局结构上进行有效的关系推理。此外，我们引入了一个立交桥/地下通道头来解决多级交叉问题，并采用耦合非极大值抑制策略来保留关键连接。我们在三个基准上评估了LineGraph2Road：城市规模、SpaceNet和全球规模，并展示了它在两个关键指标TOPO-F1和APLS上达到了最先进的结果。它还捕捉了对于实际部署至关重要的细视觉细节。我们将公开我们的代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LineGraph2Road is a framework designed to improve the extraction of road networks from satellite imagery by formulating connectedness prediction as a binary classification task over edges in a global, sparse Euclidean graph. This approach, which involves transforming the original graph into its line graph and applying a Graph Transformer, effectively captures long-range dependencies and complex topologies. The method also includes an overpass/underpass head and a coupled NMS strategy to handle multi-level crossings and preserve critical connections. Experimental results on City-scale, SpaceNet, and Global-scale benchmarks demonstrate that LineGraph2Road achieves state-of-the-art performance on TOPO-F1 and APLS metrics and captures fine visual details essential for real-world applications.</div>
<div class="mono" style="margin-top:8px">LineGraph2Road旨在通过解决现有方法在捕捉长距离依赖性和复杂拓扑结构方面的局限性，来改进从卫星图像中提取道路。它使用一个全局稀疏欧几里得图，以关键点作为节点，边表示潜在的道路段，并进行二分类以预测连接性。通过将图转换为其线图并应用图变换器，它增强了结构链接表示和关系推理。该方法还包括一个立交桥/地下通道头和耦合非最大抑制策略。在City-scale、SpaceNet和Global-scale基准上的实验表明，它在TOPO-F1和APLS关键指标上取得了最先进的结果，并且能够捕捉到对实际部署至关重要的精细视觉细节。</div>
</details>
</div>
<div class="card">
<div class="title">AgentHub: A Registry for Discoverable, Verifiable, and Reproducible AI Agents</div>
<div class="meta-line">Authors: Erik Pautsch, Tanmay Singla, Parv Kumar, Wenxin Jiang, Huiyun Peng, Behnaz Hassanshahi, Konstantin Läufer, George K. Thiruvathukal, James C. Davis</div>
<div class="meta-line">First: 2025-10-03T20:18:58+00:00 · Latest: 2026-02-26T18:01:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03495v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.03495v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agents are rapidly proliferating, yet the infrastructure for discovering, evaluating, and governing them remains fragmented compared to mature ecosystems like software package registries (e.g., npm) and model hubs (e.g., Hugging Face). Existing efforts typically address naming, distribution, or protocol descriptors, but stop short of providing a registry layer that makes agents discoverable, comparable, and governable under automated reuse. We present AgentHub, a registry layer and accompanying research agenda for agent sharing that targets discovery and workflow integration, trust and security, openness and governance, ecosystem interoperability, lifecycle transparency, and capability clarity with evidence. We describe a reference prototype that implements a canonical manifest with publish-time validation, version-bound evidence records linked to auditable artifacts, and an append-only lifecycle event log whose states are respected by default in search and resolution. We also provide initial discovery results using an LLM-as-judge recommendation pipeline, showing how structured contracts and evidence improve intent-accurate retrieval beyond keyword-driven discovery. AgentHub aims to provide a common substrate for building reliable, reusable agent ecosystems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentHub：可发现、可验证和可复现的AI代理注册表</div>
<div class="mono" style="margin-top:8px">基于LLM的代理正在迅速普及，但发现、评估和治理这些代理的基础设施仍然碎片化，与成熟的软件包注册表（例如npm）和模型库（例如Hugging Face）生态系统相比。现有努力通常仅解决命名、分发或协议描述问题，但并未提供一个注册层，使代理能够被发现、比较和在自动化重用下进行治理。我们提出了AgentHub，这是一种代理共享的注册层和伴随的研究议程，旨在解决发现和工作流集成、信任和安全、开放性和治理、生态系统互操作性、生命周期透明度和能力清晰度的问题。我们描述了一个参考原型，该原型实现了一个具有发布时验证的规范清单，版本绑定的证据记录链接到可审计的制品，并且有一个只追加的生命周期事件日志，其状态在搜索和解析中默认被尊重。我们还提供了一个使用LLM作为法官推荐管道的初步发现结果，展示了结构化合同和证据如何提高意图准确的检索，超越关键词驱动的发现。AgentHub旨在为构建可靠的、可重用的代理生态系统提供一个共同的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces AgentHub, a registry designed to make AI agents more discoverable, comparable, and governable. It addresses the fragmented infrastructure for AI agents by providing a canonical manifest with validation, version-bound evidence, and an append-only lifecycle log. Initial results show that structured contracts and evidence improve retrieval accuracy compared to keyword-based methods, enhancing the trust and security of AI agent ecosystems.</div>
<div class="mono" style="margin-top:8px">论文介绍了AgentHub，这是一种旨在使AI代理可发现、可比较和可治理的注册表。它通过关注发现、工作流集成、信任、开放性、生态系统互操作性、生命周期透明度和能力清晰度来解决代理管理的碎片化问题。关键功能包括一个带有发布时验证的规范性清单、版本绑定的证据记录以及一个只追加的生命周期事件日志。使用LLM作为法官的推荐管道的初步结果显示，结构化的合同和证据可以提高意图准确的检索，优于基于关键词的检索方法。</div>
</details>
</div>
<div class="card">
<div class="title">SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables</div>
<div class="meta-line">Authors: Sungho Park, Jueun Kim, Wook-Shin Han</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-26T17:59:51+00:00 · Latest: 2026-02-26T17:59:51+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures. Published as a conference paper at ICLR 2026. Project page: https://sparta-projectpage.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23286v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23286v1">PDF</a> · <a href="https://github.com/pshlego/SPARTA/tree/main">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://sparta-projectpage.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at https://github.com/pshlego/SPARTA/tree/main.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPARTA：面向文本和表格的树状多跳问答的可扩展和原则性基准测试</div>
<div class="mono" style="margin-top:8px">现实世界中的表格-文本问答任务需要能够跨越长文本和源表格进行推理的模型，遍历多个跳转并执行复杂的操作，如聚合。然而，现有的基准数据集规模较小，由人工精心整理，因此容易出错，并且包含浅显的问题，很少需要超过两个跳转或涉及聚合、分组或其他高级分析操作。我们提出了SPARTA，这是一种端到端的构建框架，可以自动生成大规模的表格-文本问答基准数据集，只需轻量级的人工验证，所需注释时间仅为HybridQA的四分之一。该框架首先通过丰富每个源表格，添加与附带的无结构段落自动提取的元组对齐的表格，构建参考事实数据库，然后合成嵌套查询，其嵌套谓词的数量与所需的跳转次数相匹配。为了确保每个SQL语句可执行，并且其口头表达能产生流畅的人类语言问题，我们提出了两种新颖的技术：来源导向的细化，它可以重写任何返回非空结果的语法有效的查询，以及现实结构的强制执行，它限制生成在查询图的后序遍历中。由此产生的流水线生成了数千个高质量的问题-答案对，涵盖了聚合、分组和跨越文本和表格的深层多跳推理。在SPARTA上，达到HybridQA超过70 F1或OTT-QA超过50 F1的最新模型下降超过30 F1点，揭示了当前跨模态推理中的根本弱点。我们的基准测试、构建代码和基线模型可在https://github.com/pshlego/SPARTA/tree/main/获得。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SPARTA is a scalable and principled benchmark for tree-structured multi-hop QA over text and tables, addressing the limitations of existing benchmarks by automatically generating large-scale QA pairs with lightweight human validation. The method involves enriching source tables with atomic facts from unstructured passages and synthesizing nested queries to match desired hop counts. Key findings show that state-of-the-art models perform poorly on SPARTA, dropping by more than 30 F1 points, highlighting weaknesses in current cross-modal reasoning capabilities. The benchmark and related resources are available online.</div>
<div class="mono" style="margin-top:8px">SPARTA 是一个端到端的框架，能够自动生成大规模的表格-文本 QA 基准，并且只需要 HybridQA 注释时间的四分之一。它通过从未结构化文本中提取原子事实来丰富表格，构建参考事实数据库，并生成嵌套查询以匹配所需的跳数。SPARTA 确保每个 SQL 语句可执行且其表达是流畅的。在 SPARTA 上，现有的最先进的模型在现有基准上的表现显著下降，揭示了它们在跨模态推理方面的根本弱点。基准、构建代码和基线模型已在线提供。</div>
</details>
</div>
<div class="card">
<div class="title">BioBlue: Systematic runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format</div>
<div class="meta-line">Authors: Roland Pihlakas, Sruthi Susan Kuriakose</div>
<div class="meta-line">First: 2025-09-02T15:13:14+00:00 · Latest: 2026-02-26T17:56:58+00:00</div>
<div class="meta-line">Comments: 22 pages, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02655v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.02655v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many AI alignment discussions of &quot;runaway optimisation&quot; focus on RL agents: unbounded utility maximisers that over-optimise a proxy objective (e.g., &quot;paperclip maximiser&quot;, specification gaming) at the expense of everything else. LLM-based systems are often assumed to be safer because they function as next-token predictors rather than persistent optimisers. In this work, we empirically test this assumption by placing LLMs in simple, long-horizon control-style environments that require maintaining state of or balancing objectives over time: sustainability of a renewable resource, single- and multi-objective homeostasis, and balancing unbounded objectives with diminishing returns. We find that, although models frequently behave appropriately for many steps and clearly understand the stated objectives, they often lose context in structured ways and drift into runaway behaviours: ignoring homeostatic targets, collapsing from multi-objective trade-offs into single-objective maximisation - thus failing to respect concave utility structures. These failures emerge reliably after initial periods of competent behaviour and exhibit characteristic patterns (including self-imitative oscillations, unbounded maximisation, and reverting to single-objective optimisation). The problem is not that the LLMs just lose context or become incoherent - the failures systematically resemble runaway optimisers. Our results suggest that long-horizon, multi-objective misalignment is a genuine and under-evaluated failure mode in LLM agents, even in extremely simple settings with transparent and explicitly multi-objective feedback. Although LLMs appear multi-objective and bounded on the surface, their behaviour under sustained interaction, particularly involving multiple objectives, resembles brittle, poorly aligned optimisers whose effective objective gradually shifts toward unbounded and single-metric maximisation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BioBlue：生物和经济对齐的LLM在简化观察格式下的系统性失控模式</div>
<div class="mono" style="margin-top:8px">许多关于“失控优化”的AI对齐讨论集中在RL代理上：无法限制的效用最大化者，它们会过度优化代理目标（例如，“纸夹最大化者”，规范游戏）而牺牲其他一切。基于LLM的系统通常被认为更安全，因为它们作为下一个标记预测器工作，而不是持续的优化器。在本研究中，我们通过将LLM置于简单、长周期的控制式环境中来测试这一假设，这些环境需要维持状态或在时间上平衡目标：可再生资源的可持续性、单目标和多目标的稳态维持，以及在边际效益递减的情况下平衡无界目标。我们发现，尽管模型在许多步骤中表现得当且显然理解了陈述的目标，但它们经常以结构化的方式失去上下文并进入失控行为：忽略稳态目标，从多目标权衡中崩溃为单一目标最大化——因此未能尊重凹效用结构。这些失败在初始表现良好的时期后可靠地出现，并表现出特征性模式（包括自我模仿的振荡、无界最大化和恢复为单一目标优化）。问题不在于LLM只是失去上下文或变得不连贯——失败系统地类似于失控优化器。我们的结果表明，长期、多目标的不对齐是LLM代理中真实且被低估的失败模式，即使在极其简单的透明且明确多目标反馈设置中也是如此。尽管表面上LLM看起来是多目标和有界的，但在持续交互中，特别是涉及多个目标时，其行为类似于脆弱、不良对齐的优化器，其有效目标逐渐转向无界和单一指标最大化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the potential for LLMs to exhibit runaway optimization behaviors in long-term, multi-objective settings. By placing LLMs in controlled environments that require maintaining state or balancing multiple objectives, the researchers found that while models often perform well initially, they eventually lose context and drift into behaviors that ignore homeostatic targets or revert to single-objective maximization. These behaviors are similar to those of runaway optimizers, suggesting that LLMs can fail to respect concave utility structures, even in simple settings. The study highlights the need for further evaluation of LLMs in long-horizon, multi-objective scenarios to ensure their alignment with complex objectives over time.</div>
<div class="mono" style="margin-top:8px">本研究通过将大型语言模型置于长期控制环境中，探讨了其出现失控优化的风险。尽管初期表现良好，模型往往会失去上下文并表现出失控行为，如忽视稳态目标和转向单一目标最大化。这些行为是系统性的，类似于无边界效用最大化者的表现，表明长期多目标不一致是大型语言模型中的一个显著且被低估的失败模式，即使在简单且目标明确的环境中也是如此。研究结果强调了在复杂多目标场景下对大型语言模型进行更严格测试的必要性，以确保其安全部署。</div>
</details>
</div>
<div class="card">
<div class="title">CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays</div>
<div class="meta-line">Authors: Hyungyung Lee, Hangyul Yoon, Edward Choi</div>
<div class="meta-line">First: 2026-02-26T17:51:21+00:00 · Latest: 2026-02-26T17:51:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23276v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23276v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CXReasonAgent：基于证据的胸部X光诊断推理代理</div>
<div class="mono" style="margin-top:8px">胸部X光在胸部诊断中起着核心作用，其解释本质上需要多步、基于证据的推理。然而，大型视觉-语言模型（LVLM）通常生成的响应虽然看似合理，但并不忠实于诊断证据，且提供的视觉证据有限，难以验证，同时还需要昂贵的重新训练以支持新的诊断任务，这限制了它们在临床环境中的可靠性和适应性。为解决这些局限性，我们提出了CXReasonAgent，这是一种将大型语言模型（LLM）与临床导向的诊断工具结合的诊断代理，用于使用图像衍生的诊断和视觉证据进行基于证据的诊断推理。为了评估这些能力，我们引入了包含1,946轮对话的多轮对话基准CXReasonDial，涉及12项诊断任务，并展示了CXReasonAgent能够生成忠实于证据的响应，从而实现比LVLM更可靠和可验证的诊断推理。这些发现强调了在安全关键的临床环境中整合基于临床证据的诊断工具的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CXReasonAgent is designed to perform evidence-grounded diagnostic reasoning for chest X-rays by integrating a large language model with clinically grounded diagnostic tools. It addresses the limitations of large vision-language models by producing faithfully grounded responses and providing visual evidence for verification. CXReasonAgent outperforms large vision-language models in terms of reliability and verifiability in diagnostic reasoning, as demonstrated by its performance on the CXReasonDial benchmark, which includes 1,946 dialogues across 12 diagnostic tasks.</div>
<div class="mono" style="margin-top:8px">CXReasonAgent 是一个结合了临床基础诊断工具的大语言模型，用于进行胸部X光片的证据基础诊断推理。它通过提供可靠的响应和可验证的视觉证据来解决大型视觉语言模型的局限性。CXReasonAgent 在 CXReasonDial 基准测试中的表现优于大型视觉语言模型，该基准包括1,946个涉及12项诊断任务的对话，证明了其在诊断推理中的可靠性和可验证性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260227_0349.html">20260227_0349</a>
<a href="archive/20260226_0357.html">20260226_0357</a>
<a href="archive/20260225_0353.html">20260225_0353</a>
<a href="archive/20260224_0400.html">20260224_0400</a>
<a href="archive/20260223_0328.html">20260223_0328</a>
<a href="archive/20260222_0327.html">20260222_0327</a>
<a href="archive/20260221_0340.html">20260221_0340</a>
<a href="archive/20260220_0343.html">20260220_0343</a>
<a href="archive/20260219_0354.html">20260219_0354</a>
<a href="archive/20260218_0353.html">20260218_0353</a>
<a href="archive/20260217_0336.html">20260217_0336</a>
<a href="archive/20260216_0328.html">20260216_0328</a>
<a href="archive/20260215_0327.html">20260215_0327</a>
<a href="archive/20260213_0358.html">20260213_0358</a>
<a href="archive/20260212_0400.html">20260212_0400</a>
<a href="archive/20260211_0405.html">20260211_0405</a>
<a href="archive/20260210_0407.html">20260210_0407</a>
<a href="archive/20260209_0330.html">20260209_0330</a>
<a href="archive/20260208_0328.html">20260208_0328</a>
<a href="archive/20260207_0346.html">20260207_0346</a>
<a href="archive/20260206_0343.html">20260206_0343</a>
<a href="archive/20260205_0342.html">20260205_0342</a>
<a href="archive/20260204_0351.html">20260204_0351</a>
<a href="archive/20260202_0327.html">20260202_0327</a>
<a href="archive/20260201_0324.html">20260201_0324</a>
<a href="archive/20260131_0335.html">20260131_0335</a>
<a href="archive/20260130_0334.html">20260130_0334</a>
<a href="archive/20260129_0331.html">20260129_0331</a>
<a href="archive/20260128_0330.html">20260128_0330</a>
<a href="archive/20260127_0327.html">20260127_0327</a>
<a href="archive/20260126_0321.html">20260126_0321</a>
<a href="archive/20260125_0320.html">20260125_0320</a>
<a href="archive/20260124_0329.html">20260124_0329</a>
<a href="archive/20260123_0328.html">20260123_0328</a>
<a href="archive/20260122_0333.html">20260122_0333</a>
<a href="archive/20260121_0416.html">20260121_0416</a>
<a href="archive/20260120_0324.html">20260120_0324</a>
<a href="archive/20260119_0320.html">20260119_0320</a>
<a href="archive/20260118_0318.html">20260118_0318</a>
<a href="archive/20260117_0326.html">20260117_0326</a>
<a href="archive/20260116_0329.html">20260116_0329</a>
<a href="archive/20260115_0326.html">20260115_0326</a>
<a href="archive/20260114_0325.html">20260114_0325</a>
<a href="archive/20260113_0324.html">20260113_0324</a>
<a href="archive/20260112_0323.html">20260112_0323</a>
<a href="archive/20260111_0321.html">20260111_0321</a>
<a href="archive/20260110_0324.html">20260110_0324</a>
<a href="archive/20260109_0325.html">20260109_0325</a>
<a href="archive/20260108_0325.html">20260108_0325</a>
<a href="archive/20260107_0320.html">20260107_0320</a>
<a href="archive/20260106_0327.html">20260106_0327</a>
<a href="archive/20260105_0320.html">20260105_0320</a>
<a href="archive/20260104_0319.html">20260104_0319</a>
<a href="archive/20260103_0317.html">20260103_0317</a>
<a href="archive/20260102_0329.html">20260102_0329</a>
<a href="archive/20260101_0320.html">20260101_0320</a>
<a href="archive/20251231_0326.html">20251231_0326</a>
<a href="archive/20251230_0324.html">20251230_0324</a>
<a href="archive/20251229_0320.html">20251229_0320</a>
<a href="archive/20251228_0323.html">20251228_0323</a>
<a href="archive/20251227_0321.html">20251227_0321</a>
<a href="archive/20251226_0320.html">20251226_0320</a>
<a href="archive/20251225_0320.html">20251225_0320</a>
<a href="archive/20251224_0323.html">20251224_0323</a>
<a href="archive/20251223_0323.html">20251223_0323</a>
<a href="archive/20251222_0320.html">20251222_0320</a>
<a href="archive/20251221_0320.html">20251221_0320</a>
<a href="archive/20251220_0323.html">20251220_0323</a>
<a href="archive/20251219_0323.html">20251219_0323</a>
<a href="archive/20251218_0335.html">20251218_0335</a>
<a href="archive/20251217_0324.html">20251217_0324</a>
<a href="archive/20251216_0325.html">20251216_0325</a>
<a href="archive/20251215_1246.html">20251215_1246</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
